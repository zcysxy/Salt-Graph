tags:: [[Computer Science - Machine Learning]], [[Statistics - Machine Learning]]
date:: [[2020-02-07]]
extra:: arXiv: 2002.02794
title:: @Reward-Free Exploration for Reinforcement Learning
item-type:: [[journalArticle]]
access-date:: 2021-08-18T15:18:09Z
original-title:: Reward-Free Exploration for Reinforcement Learning
language:: en
url:: http://arxiv.org/abs/2002.02794
publication-title:: "arXiv:2002.02794 [cs, stat]"
authors:: [[Chi Jin]], [[Akshay Krishnamurthy]], [[Max Simchowitz]], [[Tiancheng Yu]]
library-catalog:: arXiv.org
links:: [Local library](zotero://select/library/items/QI2Y8JBX), [Web library](https://www.zotero.org/users/6737550/items/QI2Y8JBX)

- [[Abstract]]
	- Exploration is widely regarded as one of the most challenging aspects of reinforcement learning (RL), with many naive approaches succumbing to exponential sample complexity. To isolate the challenges of exploration, we propose a new “reward-free RL” framework. In the exploration phase, the agent ﬁrst collects trajectories from an MDP M without a pre-speciﬁed reward function. After exploration, it is tasked with computing near-optimal policies under for M for a collection of given reward functions. This framework is particularly suitable when there are many reward functions of interest, or when the reward function is shaped by an external agent to elicit desired behavior.
- [[Attachments]]
	- [Reward-Free Exploration for Reinforcement Learning_Jin et al CJ et al_.pdf](zotero://select/library/items/669PFNMP) {{zotero-linked-file "attachments:CS/RL/Reward-Free Exploration for Reinforcement Learning_Jin et al CJ et al_.pdf"}}