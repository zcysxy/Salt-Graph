{:highlights [{:id #uuid "6120a96b-8f17-4533-aa8c-800786326cd8", :page 1, :position {:bounding {:x1 105.85417175292969, :y1 327.91668701171875, :x2 547.3171997070312, :y2 415.66668701171875, :width 652.8, :height 844.8}, :rects ({:x1 121.85417175292969, :y1 327.91668701171875, :x2 547.1824340820312, :y2 339.25, :width 652.8, :height 844.8} {:x1 105.85417175292969, :y1 340.5833435058594, :x2 546.98291015625, :y2 351.91668701171875, :width 652.8, :height 844.8} {:x1 105.85417175292969, :y1 353.38543701171875, :x2 547.0615844726562, :y2 364.71875, :width 652.8, :height 844.8} {:x1 105.85417175292969, :y1 366.1875, :x2 546.3621826171875, :y2 377.5208435058594, :width 652.8, :height 844.8} {:x1 105.85417175292969, :y1 378.85418701171875, :x2 547.3171997070312, :y2 390.1875, :width 652.8, :height 844.8} {:x1 105.85417175292969, :y1 391.65625, :x2 547.0570678710938, :y2 402.9895935058594, :width 652.8, :height 844.8} {:x1 105.85417175292969, :y1 404.3333435058594, :x2 410.9412536621094, :y2 415.66668701171875, :width 652.8, :height 844.8}), :page 1}, :content {:text "Exploration is widely regarded as one of the most challenging aspects of reinforcement learning (RL),with many naive approaches succumbing to exponential sample complexity. To isolate the challengesof exploration, we propose a new “reward-free RL” framework. In the exploration phase, the agent firstcollects trajectories from an MDPMwithouta pre-specified reward function. After exploration, it istasked with computing near-optimal policies under forMfor a collection of given reward functions.This framework is particularly suitable when there are manyreward functions of interest, or when thereward function is shaped by an external agent to elicit desired behavior"}, :properties {:color "yellow"}} {:id #uuid "6120a9b0-cb7a-45b8-b13f-dfd73efcad37", :page 1, :position {:bounding {:x1 105.85417175292969, :y1 417.125, :x2 547.1611328125, :y2 505.01043701171875, :width 652.8, :height 844.8}, :rects ({:x1 121.85417175292969, :y1 417.125, :x2 547.1611328125, :y2 428.4583435058594, :width 652.8, :height 844.8} {:x1 105.85417175292969, :y1 429.9270935058594, :x2 547.0345458984375, :y2 441.2604217529297, :width 652.8, :height 844.8} {:x1 105.85417175292969, :y1 442.6041717529297, :x2 547.0662841796875, :y2 453.9375, :width 652.8, :height 844.8} {:x1 105.85417175292969, :y1 455.40625, :x2 547.1023559570312, :y2 466.7395935058594, :width 652.8, :height 844.8} {:x1 105.85417175292969, :y1 468.19793701171875, :x2 546.8598022460938, :y2 479.53125, :width 652.8, :height 844.8} {:x1 105.85417175292969, :y1 480.875, :x2 546.9777221679688, :y2 492.2083435058594, :width 652.8, :height 844.8} {:x1 105.85417175292969, :y1 493.6770935058594, :x2 134.31759643554688, :y2 505.01043701171875, :width 652.8, :height 844.8}), :page 1}, :content {:text "We give an efficient algorithm that conducts ̃O(S2Apoly(H)/ǫ2)episodes of exploration and re-turnsǫ-suboptimal policies for anarbitrarynumber of reward functions. We achieve this by findingexploratory policies that visit each “significant” state with probability proportional to its maximum vis-itation probability underanypossible policy. Moreover, our planning procedure can be instantiated byany black-box approximate planner, such as value iterationor natural policy gradient. We also give anearly-matchingΩ(S2AH2/ǫ2)lower bound, demonstrating the near-optimality of our algorithm in thissetting"}, :properties {:color "blue"}} {:id #uuid "6120a9ec-481c-4026-b6da-797b215dee13", :page 1, :position {:bounding {:x1 66, :y1 485.33333587646484, :x2 507, :y2 612.3333358764648, :width 571.1999999999999, :height 739.1999999999999}, :rects (), :page 1}, :content {:text "[:span]", :image 1629530602145}, :properties {:color "green"}}]}